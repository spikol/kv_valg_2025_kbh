{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DR Municipal Election Candidate Scraper\n",
    "This notebook scrapes candidate information from DR's municipal election pages.\n",
    "\n",
    "## Installation\n",
    "Run this cell first to install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium beautifulsoup4 pandas webdriver-manager requests lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize WebDriver\n",
    "Using Chrome in headless mode for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_driver(headless=True):\n",
    "    \"\"\"Set up Chrome WebDriver with options\"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--window-size=1920,1080')\n",
    "    options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "    \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "# Initialize driver\n",
    "driver = setup_driver(headless=True)\n",
    "print(\"WebDriver initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Extract Candidate Links from a Municipality Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_links(municipality_url, driver, wait_time=10):\n",
    "    \"\"\"\n",
    "    Extract all candidate links from a municipality page\n",
    "    \n",
    "    Args:\n",
    "        municipality_url: URL of the municipality page (e.g., .../124)\n",
    "        driver: Selenium WebDriver instance\n",
    "        wait_time: Time to wait for page elements to load\n",
    "    \n",
    "    Returns:\n",
    "        List of candidate URLs\n",
    "    \"\"\"\n",
    "    print(f\"\\nFetching candidate links from: {municipality_url}\")\n",
    "    driver.get(municipality_url)\n",
    "    \n",
    "    # Wait for page to load\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Try multiple selectors to find candidate links\n",
    "    candidate_links = []\n",
    "    \n",
    "    try:\n",
    "        # Wait for candidate elements to load\n",
    "        WebDriverWait(driver, wait_time).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"a\"))\n",
    "        )\n",
    "        \n",
    "        # Get page source and parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        \n",
    "        # Look for links containing '/kandidater/kommune/'\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if '/kandidater/kommune/' in href:\n",
    "                full_url = urljoin('https://www.dr.dk', href)\n",
    "                if full_url not in candidate_links:\n",
    "                    candidate_links.append(full_url)\n",
    "        \n",
    "        print(f\"Found {len(candidate_links)} candidate links\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error finding candidate links: {e}\")\n",
    "    \n",
    "    return candidate_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Scrape Individual Candidate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_candidate_data(candidate_url, driver, wait_time=10):\n",
    "    \"\"\"\n",
    "    Scrape data from an individual candidate page\n",
    "    \n",
    "    Args:\n",
    "        candidate_url: URL of the candidate page\n",
    "        driver: Selenium WebDriver instance\n",
    "        wait_time: Time to wait for page elements to load\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing candidate data\n",
    "    \"\"\"\n",
    "    print(f\"Scraping: {candidate_url}\")\n",
    "    \n",
    "    try:\n",
    "        driver.get(candidate_url)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Get page source\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        \n",
    "        # Extract candidate ID and name from URL\n",
    "        url_parts = candidate_url.split('/')[-1]\n",
    "        candidate_id = url_parts.split('-')[0] if '-' in url_parts else ''\n",
    "        \n",
    "        # Extract page title (contains name and party)\n",
    "        page_title = soup.find('title')\n",
    "        title_text = page_title.text if page_title else ''\n",
    "        \n",
    "        # Parse name and party from title\n",
    "        name = ''\n",
    "        party = ''\n",
    "        municipality = ''\n",
    "        \n",
    "        if title_text:\n",
    "            # Format: \"Name (Party) Municipality | KV25 | DR\"\n",
    "            parts = title_text.split('|')[0].strip()\n",
    "            if '(' in parts and ')' in parts:\n",
    "                name_part = parts.split('(')[0].strip()\n",
    "                party_part = parts.split('(')[1].split(')')[0].strip()\n",
    "                municipality_part = parts.split(')')[1].strip() if len(parts.split(')')) > 1 else ''\n",
    "                \n",
    "                name = name_part\n",
    "                party = party_part\n",
    "                municipality = municipality_part\n",
    "        \n",
    "        # Extract policy priorities (numbered items)\n",
    "        priorities = []\n",
    "        \n",
    "        # Look for numbered content\n",
    "        text_content = soup.get_text(separator='\\n', strip=True)\n",
    "        lines = text_content.split('\\n')\n",
    "        \n",
    "        current_priority = {}\n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            # Check if line is a number (priority number)\n",
    "            if line.isdigit() and int(line) <= 10:\n",
    "                # If we have a previous priority, save it\n",
    "                if current_priority:\n",
    "                    priorities.append(current_priority)\n",
    "                current_priority = {'number': int(line), 'text': ''}\n",
    "            elif current_priority and line and not line.isdigit():\n",
    "                # Add text to current priority\n",
    "                if current_priority['text']:\n",
    "                    current_priority['text'] += ' '\n",
    "                current_priority['text'] += line\n",
    "        \n",
    "        # Add last priority\n",
    "        if current_priority:\n",
    "            priorities.append(current_priority)\n",
    "        \n",
    "        # Extract any contact information\n",
    "        email = ''\n",
    "        phone = ''\n",
    "        website = ''\n",
    "        \n",
    "        # Look for email\n",
    "        email_match = re.search(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', text_content)\n",
    "        if email_match:\n",
    "            email = email_match.group(0)\n",
    "        \n",
    "        # Look for phone numbers\n",
    "        phone_match = re.search(r'\\+?\\d[\\d\\s-]{7,}\\d', text_content)\n",
    "        if phone_match:\n",
    "            phone = phone_match.group(0)\n",
    "        \n",
    "        # Compile all data\n",
    "        candidate_data = {\n",
    "            'url': candidate_url,\n",
    "            'candidate_id': candidate_id,\n",
    "            'name': name,\n",
    "            'party': party,\n",
    "            'municipality': municipality,\n",
    "            'email': email,\n",
    "            'phone': phone,\n",
    "            'website': website,\n",
    "            'priorities': priorities,\n",
    "            'num_priorities': len(priorities)\n",
    "        }\n",
    "        \n",
    "        return candidate_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {candidate_url}: {e}\")\n",
    "        return {\n",
    "            'url': candidate_url,\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Scraping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_municipality(municipality_url, max_candidates=None):\n",
    "    \"\"\"\n",
    "    Scrape all candidates from a municipality\n",
    "    \n",
    "    Args:\n",
    "        municipality_url: URL of the municipality page\n",
    "        max_candidates: Maximum number of candidates to scrape (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        List of candidate data dictionaries\n",
    "    \"\"\"\n",
    "    # Get candidate links\n",
    "    candidate_links = get_candidate_links(municipality_url, driver)\n",
    "    \n",
    "    if max_candidates:\n",
    "        candidate_links = candidate_links[:max_candidates]\n",
    "    \n",
    "    # Scrape each candidate\n",
    "    all_candidates = []\n",
    "    for i, link in enumerate(candidate_links, 1):\n",
    "        print(f\"\\nProcessing candidate {i}/{len(candidate_links)}\")\n",
    "        candidate_data = scrape_candidate_data(link, driver)\n",
    "        all_candidates.append(candidate_data)\n",
    "        \n",
    "        # Be polite to the server\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return all_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Scrape Municipality 124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example municipality URL\n",
    "municipality_url = \"https://www.dr.dk/nyheder/politik/kommunalvalg/din-stemmeseddel/124\"\n",
    "\n",
    "# Scrape candidates (limit to 5 for testing)\n",
    "candidates = scrape_municipality(municipality_url, max_candidates=5)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Scraped {len(candidates)} candidates\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to DataFrame and Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with basic info\n",
    "df_basic = pd.DataFrame([{\n",
    "    'candidate_id': c['candidate_id'],\n",
    "    'name': c['name'],\n",
    "    'party': c['party'],\n",
    "    'municipality': c['municipality'],\n",
    "    'email': c['email'],\n",
    "    'phone': c['phone'],\n",
    "    'num_priorities': c['num_priorities'],\n",
    "    'url': c['url']\n",
    "} for c in candidates if 'error' not in c])\n",
    "\n",
    "print(\"\\nBasic Candidate Information:\")\n",
    "display(df_basic)\n",
    "\n",
    "# Show detailed priorities for first candidate\n",
    "if candidates and 'priorities' in candidates[0]:\n",
    "    print(f\"\\n\\nDetailed Priorities for {candidates[0].get('name', 'Unknown')}:\")\n",
    "    for priority in candidates[0]['priorities']:\n",
    "        print(f\"\\n{priority['number']}. {priority['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand Priorities into Separate DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create expanded DataFrame with one row per priority\n",
    "priority_rows = []\n",
    "\n",
    "for candidate in candidates:\n",
    "    if 'error' not in candidate and 'priorities' in candidate:\n",
    "        for priority in candidate['priorities']:\n",
    "            priority_rows.append({\n",
    "                'candidate_id': candidate['candidate_id'],\n",
    "                'name': candidate['name'],\n",
    "                'party': candidate['party'],\n",
    "                'municipality': candidate['municipality'],\n",
    "                'priority_number': priority['number'],\n",
    "                'priority_text': priority['text']\n",
    "            })\n",
    "\n",
    "df_priorities = pd.DataFrame(priority_rows)\n",
    "\n",
    "print(\"\\nExpanded Priorities DataFrame:\")\n",
    "display(df_priorities.head(10))\n",
    "\n",
    "print(f\"\\nTotal priorities: {len(df_priorities)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "df_basic.to_csv('candidates_basic.csv', index=False, encoding='utf-8')\n",
    "df_priorities.to_csv('candidates_priorities.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Save raw data to JSON\n",
    "with open('candidates_raw.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(candidates, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Data saved to:\")\n",
    "print(\"- candidates_basic.csv\")\n",
    "print(\"- candidates_priorities.csv\")\n",
    "print(\"- candidates_raw.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Scrape Multiple Municipalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_multiple_municipalities(municipality_ids, max_candidates_per_municipality=None):\n",
    "    \"\"\"\n",
    "    Scrape candidates from multiple municipalities\n",
    "    \n",
    "    Args:\n",
    "        municipality_ids: List of municipality IDs (e.g., [124, 101, 147])\n",
    "        max_candidates_per_municipality: Max candidates per municipality\n",
    "    \n",
    "    Returns:\n",
    "        Combined list of all candidates\n",
    "    \"\"\"\n",
    "    all_candidates = []\n",
    "    \n",
    "    for muni_id in municipality_ids:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing Municipality: {muni_id}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        muni_url = f\"https://www.dr.dk/nyheder/politik/kommunalvalg/din-stemmeseddel/{muni_id}\"\n",
    "        candidates = scrape_municipality(muni_url, max_candidates_per_municipality)\n",
    "        all_candidates.extend(candidates)\n",
    "        \n",
    "        print(f\"\\nTotal candidates so far: {len(all_candidates)}\")\n",
    "        time.sleep(2)  # Be polite between municipalities\n",
    "    \n",
    "    return all_candidates\n",
    "\n",
    "# Example: Scrape multiple municipalities\n",
    "# municipality_ids = [124, 101, 147]  # Add your municipality IDs\n",
    "# all_candidates = scrape_multiple_municipalities(municipality_ids, max_candidates_per_municipality=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count candidates by party\n",
    "print(\"Candidates by Party:\")\n",
    "print(df_basic['party'].value_counts())\n",
    "\n",
    "print(\"\\nCandidates by Municipality:\")\n",
    "print(df_basic['municipality'].value_counts())\n",
    "\n",
    "# Average number of priorities per candidate\n",
    "print(f\"\\nAverage priorities per candidate: {df_basic['num_priorities'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis of Priorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most common words in priorities\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Combine all priority texts\n",
    "all_priority_text = ' '.join(df_priorities['priority_text'].astype(str))\n",
    "\n",
    "# Simple word frequency (you can enhance this with proper NLP)\n",
    "words = re.findall(r'\\b\\w+\\b', all_priority_text.lower())\n",
    "word_freq = Counter(words)\n",
    "\n",
    "# Remove common Danish stop words (add more as needed)\n",
    "stop_words = {'og', 'i', 'til', 'at', 'det', 'er', 'en', 'for', 'med', 'pÃ¥', 'som', 'der', 'af', 'de', 'vi'}\n",
    "filtered_words = {word: count for word, count in word_freq.items() if word not in stop_words and len(word) > 3}\n",
    "\n",
    "print(\"\\nTop 20 Most Common Words in Priorities:\")\n",
    "for word, count in sorted(filtered_words.items(), key=lambda x: x[1], reverse=True)[:20]:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up: Close WebDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always close the driver when done\n",
    "driver.quit()\n",
    "print(\"WebDriver closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
