{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DR Candidate Scraper (Simple Version)\n",
    "This is a simpler version using requests + BeautifulSoup (no Selenium required).\n",
    "**Note:** This may not work if the website requires JavaScript to load content.\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4 pandas lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create session with headers\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "})\n",
    "\n",
    "print(\"Session created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Get Candidate Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_links_simple(municipality_url):\n",
    "    \"\"\"\n",
    "    Extract candidate links from municipality page using requests\n",
    "    \"\"\"\n",
    "    print(f\"\\nFetching: {municipality_url}\")\n",
    "    \n",
    "    try:\n",
    "        response = session.get(municipality_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "        \n",
    "        candidate_links = []\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if '/kandidater/kommune/' in href:\n",
    "                full_url = urljoin('https://www.dr.dk', href)\n",
    "                if full_url not in candidate_links:\n",
    "                    candidate_links.append(full_url)\n",
    "        \n",
    "        print(f\"Found {len(candidate_links)} candidate links\")\n",
    "        return candidate_links\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Scrape Candidate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_candidate_simple(candidate_url):\n",
    "    \"\"\"\n",
    "    Scrape individual candidate page\n",
    "    \"\"\"\n",
    "    print(f\"Scraping: {candidate_url}\")\n",
    "    \n",
    "    try:\n",
    "        response = session.get(candidate_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "        \n",
    "        # Extract candidate ID from URL\n",
    "        url_parts = candidate_url.split('/')[-1]\n",
    "        candidate_id = url_parts.split('-')[0] if '-' in url_parts else ''\n",
    "        \n",
    "        # Get title\n",
    "        page_title = soup.find('title')\n",
    "        title_text = page_title.text if page_title else ''\n",
    "        \n",
    "        # Parse name, party, municipality from title\n",
    "        name = ''\n",
    "        party = ''\n",
    "        municipality = ''\n",
    "        \n",
    "        if title_text:\n",
    "            parts = title_text.split('|')[0].strip()\n",
    "            if '(' in parts and ')' in parts:\n",
    "                name = parts.split('(')[0].strip()\n",
    "                party = parts.split('(')[1].split(')')[0].strip()\n",
    "                municipality = parts.split(')')[1].strip() if len(parts.split(')')) > 1 else ''\n",
    "        \n",
    "        # Extract all text content\n",
    "        text_content = soup.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        # Parse priorities\n",
    "        priorities = []\n",
    "        lines = text_content.split('\\n')\n",
    "        \n",
    "        current_priority = {}\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.isdigit() and int(line) <= 10:\n",
    "                if current_priority:\n",
    "                    priorities.append(current_priority)\n",
    "                current_priority = {'number': int(line), 'text': ''}\n",
    "            elif current_priority and line and not line.isdigit():\n",
    "                if current_priority['text']:\n",
    "                    current_priority['text'] += ' '\n",
    "                current_priority['text'] += line\n",
    "        \n",
    "        if current_priority:\n",
    "            priorities.append(current_priority)\n",
    "        \n",
    "        # Extract contact info\n",
    "        email_match = re.search(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', text_content)\n",
    "        email = email_match.group(0) if email_match else ''\n",
    "        \n",
    "        phone_match = re.search(r'\\+?\\d[\\d\\s-]{7,}\\d', text_content)\n",
    "        phone = phone_match.group(0) if phone_match else ''\n",
    "        \n",
    "        return {\n",
    "            'url': candidate_url,\n",
    "            'candidate_id': candidate_id,\n",
    "            'name': name,\n",
    "            'party': party,\n",
    "            'municipality': municipality,\n",
    "            'email': email,\n",
    "            'phone': phone,\n",
    "            'priorities': priorities,\n",
    "            'num_priorities': len(priorities)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {candidate_url}: {e}\")\n",
    "        return {'url': candidate_url, 'error': str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Scraping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_municipality_simple(municipality_url, max_candidates=None):\n",
    "    \"\"\"\n",
    "    Scrape all candidates from a municipality\n",
    "    \"\"\"\n",
    "    candidate_links = get_candidate_links_simple(municipality_url)\n",
    "    \n",
    "    if max_candidates:\n",
    "        candidate_links = candidate_links[:max_candidates]\n",
    "    \n",
    "    all_candidates = []\n",
    "    for i, link in enumerate(candidate_links, 1):\n",
    "        print(f\"\\nProcessing candidate {i}/{len(candidate_links)}\")\n",
    "        candidate_data = scrape_candidate_simple(link)\n",
    "        all_candidates.append(candidate_data)\n",
    "        time.sleep(1)  # Be polite\n",
    "    \n",
    "    return all_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with municipality 124\n",
    "municipality_url = \"https://www.dr.dk/nyheder/politik/kommunalvalg/din-stemmeseddel/124\"\n",
    "candidates = scrape_municipality_simple(municipality_url, max_candidates=5)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Scraped {len(candidates)} candidates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info DataFrame\n",
    "df = pd.DataFrame([{\n",
    "    'candidate_id': c.get('candidate_id', ''),\n",
    "    'name': c.get('name', ''),\n",
    "    'party': c.get('party', ''),\n",
    "    'municipality': c.get('municipality', ''),\n",
    "    'email': c.get('email', ''),\n",
    "    'phone': c.get('phone', ''),\n",
    "    'num_priorities': c.get('num_priorities', 0),\n",
    "    'url': c.get('url', '')\n",
    "} for c in candidates if 'error' not in c])\n",
    "\n",
    "display(df)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('candidates_simple.csv', index=False, encoding='utf-8')\n",
    "print(\"\\nSaved to candidates_simple.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priorities DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand priorities\n",
    "priority_rows = []\n",
    "for c in candidates:\n",
    "    if 'error' not in c and 'priorities' in c:\n",
    "        for p in c['priorities']:\n",
    "            priority_rows.append({\n",
    "                'candidate_id': c['candidate_id'],\n",
    "                'name': c['name'],\n",
    "                'party': c['party'],\n",
    "                'priority_number': p['number'],\n",
    "                'priority_text': p['text']\n",
    "            })\n",
    "\n",
    "df_priorities = pd.DataFrame(priority_rows)\n",
    "display(df_priorities)\n",
    "\n",
    "# Save\n",
    "df_priorities.to_csv('priorities_simple.csv', index=False, encoding='utf-8')\n",
    "print(\"\\nSaved to priorities_simple.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
